{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Hindi Task B - Ensemble XLMRobertaAttention .ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaHXdQweG2gY",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnXQrkMlG2ga",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UcEUgF4G2gc",
        "colab_type": "text"
      },
      "source": [
        "At the time of our work, we used the following library versions\n",
        "- numpy 1.18.1\n",
        "- pandas 1.0.1\n",
        "- torch 1.2.0\n",
        "- Cuda 10.0\n",
        "- python 3.7.0\n",
        "- sklearn 0.22.1\n",
        "- tqdm 4.42.1\n",
        "- nltk 3.4.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jknbt-9FG9sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/cozek/trac2020_submission"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7C7pQyHPLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install /content/transformers/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMKCJ-dG2ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/trac2020_submission/src/')\n",
        "import collections\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "from tqdm import notebook\n",
        "import importlib\n",
        "import pprint\n",
        "import nltk\n",
        "import datetime\n",
        "import os\n",
        "from argparse import Namespace\n",
        "import re\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwTBxXqG2gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import utils.general as general_utils\n",
        "import utils.trac2020 as trac_utils\n",
        "import utils.transformer.data as transformer_data_utils\n",
        "import utils.transformer.general as transformer_general_utils\n",
        "general_utils.set_seed_everywhere() #set the seed for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfdINgcG2gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6k1nQ5d4G2gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcFCAJAQG2gr",
        "colab_type": "text"
      },
      "source": [
        "## Import Optimzer and XLM Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc66UeiyG2gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import RAdam and Lookahead\n",
        "from radam.radam import RAdam\n",
        "from lookahead.optimizer import Lookahead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sF7KP8tzG2gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIAYRiqhG2gz",
        "colab_type": "text"
      },
      "source": [
        "# Set up the argspace/important_variables\n",
        "Please note that performance is suseptible to hyper parameters. We used a Nvidia Tesla V100 32GB. If you lower the batch size or change any other parameters, modules to fit your machine, you might not get the same performance as reported in our paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F6tx_VrG2gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Namespace(\n",
        "        #use cuda by default\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "        #set batch size and number of epochs\n",
        "        batch_size = 32,\n",
        "        num_epochs = 20,\n",
        "    \n",
        "        #set the learning rate\n",
        "        learning_rate = 0.0001,\n",
        "\n",
        "        #location of the train, dev and test csv\n",
        "        train_csv = '/content/trac2020_submission/data/hin/trac2_hin_train.csv',\n",
        "        dev_csv = '/content/trac2020_submission/data/hin/trac2_hin_dev.csv',\n",
        "        test_csv = '/content/trac2020_submission/data/test/trac2_hin_test.csv',\n",
        "    \n",
        "        #directory to save our models at\n",
        "        directory = './', \n",
        "        model_name = 'xlmrobeta_hin_b.pt',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY29sw-7G2g2",
        "colab_type": "text"
      },
      "source": [
        "## Load the data csv into DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKKJ-JmZG2g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_train_df =  pd.read_csv(args.train_csv)\n",
        "raw_train_df['split'] = 'train'\n",
        "print(raw_train_df.columns)\n",
        "print(raw_train_df['Sub-task A'].value_counts())\n",
        "print(raw_train_df['Sub-task B'].value_counts())\n",
        "print(f\"Size of 'train' split: {len(raw_train_df)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSM9RGfRG2g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_dev_df =  pd.read_csv(args.dev_csv)\n",
        "raw_dev_df['split'] = 'dev'\n",
        "print(raw_dev_df.columns)\n",
        "print(raw_dev_df['Sub-task A'].value_counts())\n",
        "print(raw_dev_df['Sub-task B'].value_counts())\n",
        "print(f\"Size of 'dev' split: {len(raw_dev_df)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W99Hc1Y-G2hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatinate both train and dev dfs together\n",
        "data_df = pd.concat([raw_dev_df, raw_train_df], ignore_index= True)\n",
        "data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z1dLUbuG2hD",
        "colab_type": "text"
      },
      "source": [
        "### Samples given per label size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad1zsEMPG2hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Total dev + train size = {len(data_df)}\\n')\n",
        "print(data_df['Sub-task A'].value_counts(),'\\n')\n",
        "print(data_df['Sub-task B'].value_counts(),'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hY0WnQuG2hH",
        "colab_type": "text"
      },
      "source": [
        "### Map to labels to integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xqQas0q9G2hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task_b_label_dict = {'NGEN':0, 'GEN':1}\n",
        "print(task_b_label_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7M9WRnMG2hL",
        "colab_type": "text"
      },
      "source": [
        "### Renaming the columns for our torch dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nzmWDI7G2hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df_task_b = data_df[['ID','Text','Sub-task B','split']].copy()\n",
        "data_df_task_b.columns.values[1] = 'text'\n",
        "data_df_task_b.columns.values[2] = 'label'\n",
        "data_df_task_b.loc[:,'label'] = data_df_task_b.loc[:,'label'].map(task_b_label_dict) \n",
        "data_df_task_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxb1PR7G2hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Num samples per class\")\n",
        "print(data_df_task_b.label.value_counts())\n",
        "\n",
        "print(\"\\nNum samples per split\")\n",
        "print(data_df_task_b.split.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in dev split\")\n",
        "print(data_df_task_b[data_df_task_b.split=='dev'].label.value_counts())\n",
        "\n",
        "print(\"\\nLabel counts in train split\")\n",
        "print(data_df_task_b[data_df_task_b.split=='train'].label.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYW_O7yfG2hU",
        "colab_type": "text"
      },
      "source": [
        "### We split long samples into multiple samples\n",
        "Each sample produced from from a single split will have the label of the original sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRIJwIb_G2hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split long sentences into sentences of 200 words\n",
        "data_df_task_b['text'] = data_df_task_b['text'].map(lambda x: trac_utils.chunk_sent(x,150,50))\n",
        "exploded_df = data_df_task_b.explode('text').reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNRu4kwUG2hX",
        "colab_type": "text"
      },
      "source": [
        "#### Notice how a single sample is split into two samples in the exploded_df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8CX-WCP-G2hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df_task_b[data_df_task_b.ID=='C7.849']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gJ_6MLU9G2ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exploded_df[exploded_df.ID == 'C7.849']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VZwuf3tqG2he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Samples before splitting\")\n",
        "print(data_df_task_b.split.value_counts())\n",
        "\n",
        "print(\"\\nSamples After splitting\")\n",
        "print(exploded_df.split.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKy-82YrG2hh",
        "colab_type": "text"
      },
      "source": [
        "## Create the text preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhSisD5G2hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RobertaPreprocessor():\n",
        "    \"\"\"\n",
        "    Preprocessor for adding special tokens into each sample\n",
        "    NOTE: Doesn't work perfectly.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self,transformer_tokenizer,sentence_detector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            transformer_tokenizer: Tokenizer for the transformer model\n",
        "            sentence_detector: Sentence tokenizer.\n",
        "        \"\"\"\n",
        "        self.transformer_tokenizer = transformer_tokenizer\n",
        "        self.sentence_detector = sentence_detector\n",
        "        self.bos_token = transformer_tokenizer.bos_token\n",
        "        self.sep_token = ' ' + transformer_tokenizer.sep_token + ' '\n",
        "        \n",
        "    def add_special_tokens(self, text):\n",
        "        \"\"\"\n",
        "        Adds '</s>' between each sentence and at the end of the sample.\n",
        "        Adds '<s>' at the start of the sentence.\n",
        "        \n",
        "        Args:\n",
        "            text: Text sample to add special tokens into\n",
        "        Returns:\n",
        "            text with special tokens added\n",
        "        \"\"\"\n",
        "        text = ' '.join(text.strip().split()) #clean whitespaces\n",
        "        sentences = self.sentence_detector.tokenize(text)\n",
        "        eos_added_text  = self.sep_token.join(sentences) \n",
        "        return self.bos_token +' '+ eos_added_text + ' ' + self.transformer_tokenizer.sep_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCMVmYNJHca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -c 'import nltk; nltk.download(\"punkt\")'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uX6GH1WNG2hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZCJQllG2hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_preproc = RobertaPreprocessor(xlmroberta_tokenizer, punkt_sentence_detector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKml6Hy5G2hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply the preprocessor on the exploded dataframe\n",
        "exploded_df['text'] = exploded_df['text'].map(roberta_preproc.add_special_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kRdNEy92G2hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exploded_df.loc[2].text #notice the addition of eos token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43q7GSYEG2hu",
        "colab_type": "text"
      },
      "source": [
        "### Create the Vectorizer and the torch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlI6CoLfG2hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleVectorizer():\n",
        "    \"\"\"Vectorizes Class to encode the samples into \n",
        "    their token ids and creates their respective attention masks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,tokenizer: Callable, max_seq_len: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tokenizer (Callable): transformer tokenizer\n",
        "            max_seq_len (int): Maximum sequence lenght \n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self._max_seq_len = max_seq_len\n",
        "\n",
        "    def vectorize(self,text :str):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: Text sample to vectorize\n",
        "        Returns:\n",
        "            ids: Token ids of the \n",
        "            attn: Attention masks for ids \n",
        "        \"\"\"\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=False, #already added by preprocessor\n",
        "            max_length = self._max_seq_len,\n",
        "            pad_to_max_length = True,\n",
        "        )\n",
        "        ids =  np.array(encoded['input_ids'], dtype=np.int64)\n",
        "        attn = np.array(encoded['attention_mask'], dtype=np.int64)\n",
        "        \n",
        "        return ids, attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2wGyBHOG2h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TracDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset class\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_df: pd.DataFrame,\n",
        "        tokenizer: Callable,\n",
        "        max_seq_length:int = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_df (pandas.DataFrame): df containing the labels and text\n",
        "            tokenizer (Callable): tokenizer for the transformer\n",
        "            max_seq_length (int): Maximum sequece length to work with.\n",
        "        \"\"\"\n",
        "        self.data_df = data_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if max_seq_length is None:\n",
        "            self._max_seq_length = self._get_max_len(data_df,tokenizer)\n",
        "        else:\n",
        "            self._max_seq_length = max_seq_length\n",
        "\n",
        "        self.train_df = self.data_df[self.data_df.split == 'train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.data_df[self.data_df.split == 'dev']\n",
        "        self.val_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.data_df[self.data_df.split == 'test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        \n",
        "        self._simple_vectorizer = SimpleVectorizer(tokenizer, self._max_seq_length)\n",
        "        \n",
        "        self._lookup_dict = {\n",
        "            'train': (self.train_df, self.train_size),\n",
        "            'val': (self.val_df, self.val_size),\n",
        "            'test': (self.test_df, self.test_size)\n",
        "        }\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "    \n",
        "    def _get_max_len(self,data_df: pd.DataFrame, tokenizer: Callable):\n",
        "        \"\"\"Get the maximum lenght found in the data\n",
        "        Args:\n",
        "            data_df (pandas.DataFrame): The pandas dataframe with the data\n",
        "            tokenizer (Callable): The tokenizer of the transformer\n",
        "        Returns:\n",
        "            max_len (int): Maximum length\n",
        "        \"\"\"\n",
        "        len_func = lambda x: len(self.tokenizer.encode_plus(x)['input_ids'])\n",
        "        max_len = data_df.text.map(len_func).max() \n",
        "        return max_len\n",
        "\n",
        "    \n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\"selects the splits in the dataset using a column in the dataframe \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        \n",
        "        indices, attention_masks = self._simple_vectorizer.vectorize(row.text)\n",
        "\n",
        "\n",
        "        label = row.label\n",
        "        return {'x_data': indices,\n",
        "                'x_attn_mask': attention_masks,\n",
        "                'x_index': index,\n",
        "                'y_target': label}\n",
        "    \n",
        "    \n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0izVXm7xG2h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=False, device=\"cpu\", pinned_memory = False, n_workers = 0): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last,\n",
        "                            pin_memory= pinned_memory,\n",
        "                            num_workers = n_workers,\n",
        "                            )\n",
        "    \n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        out_data_dict['x_data'] = data_dict['x_data'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_attn_mask'] = data_dict['x_attn_mask'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        out_data_dict['x_index'] = data_dict['x_index']\n",
        "        out_data_dict['y_target'] = data_dict['y_target'].to(\n",
        "            device, non_blocking= (True if pinned_memory else False) \n",
        "        )\n",
        "        yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctSoIHfG2h5",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPG1JYbDG2h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = TracDataset(\n",
        "    data_df = exploded_df,\n",
        "    tokenizer = xlmroberta_tokenizer,\n",
        "    max_seq_length = 403 #what we used\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "SbulHEyLG2h8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset._max_seq_length # make sure its safe enough for our model, i,e, < 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWqPuxZvG2iA",
        "colab_type": "text"
      },
      "source": [
        "# Creating the XLMRoberta + Attention model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsL6TvrsG2iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XLMRoBertAttention(nn.Module):\n",
        "    \"\"\"Implements Attention Head Classifier\n",
        "    on Pretrained Roberta Transformer representations.\n",
        "    Attention Head Implementation : https://www.aclweb.org/anthology/P16-2034/\n",
        "    \"\"\"\n",
        "    \n",
        "    def penalized_tanh(self,x):\n",
        "        \"\"\"\n",
        "        http://aclweb.org/anthology/D18-1472\n",
        "        \"\"\"\n",
        "        alpha = 0.25\n",
        "        return torch.max(torch.tanh(x), alpha*torch.tanh(x))\n",
        "    \n",
        "    \n",
        "    def __init__(self, model_name, num_labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_name: model name, eg, roberta-base'\n",
        "            num_labels: number of classes to classify\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.w = nn.Linear(768,1, bias=False)\n",
        "        self.bert = XLMRobertaModel.from_pretrained(model_name)\n",
        "        self.prediction_layer = nn.Linear(768, num_labels)\n",
        "        self.init_weights()\n",
        "        \n",
        "        \n",
        "    def init_weights(self):\n",
        "        \"\"\"Initializes the weights of the Attention head classifier\"\"\"\n",
        "        \n",
        "        for name, param in self.prediction_layer.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "        for name, param in self.w.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids,attention_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: sent encoded into indices\n",
        "            attention_mask: their respective attention masks\n",
        "        Returns:\n",
        "            preds: Final layer output of the model\n",
        "        \"\"\"\n",
        "        embeddings = self.bert(input_ids = input_ids,\n",
        "                  attention_mask = attention_mask)\n",
        "        H = embeddings[0] #final hidden layer outputs \n",
        "        M = self.penalized_tanh(H)\n",
        "        alpha = torch.softmax(self.w(M), dim=1)\n",
        "        r = torch.bmm(H.permute(0,2,1),alpha)\n",
        "        h_star = self.penalized_tanh(r)\n",
        "        preds = self.prediction_layer(h_star.permute(0,2,1))\n",
        "        return preds\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78_xW4uUG2iD",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O6NjugAyG2iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XLMRoBertAttention(\n",
        "    model_name = 'xlm-roberta-base',\n",
        "    num_labels = len(set(dataset.data_df.label)),\n",
        ")\n",
        "model.to(args.device) #send the model to the 'cpu' or 'gpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "AvKmZUh_G2iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "early_stopping = transformer_general_utils.EarlyStopping(patience=4)\n",
        "base_optimizer = RAdam(model.parameters(), lr = args.learning_rate, weight_decay=1e-5)\n",
        "optimizer = Lookahead(optimizer = base_optimizer, k = 6, alpha=0.5 )\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer.optimizer, factor =0.1 ,mode='max')\n",
        "\n",
        "print(f'Using LR:{args.learning_rate}\\n Early Stopping Patience: 4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLAYZmmtG2iJ",
        "colab_type": "text"
      },
      "source": [
        "# Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vC3ewUSnG2iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_state = general_utils.make_train_state() #dictionary for saving training routine information\n",
        "train_state.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr0zwdnYKNtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc81HVEZKWzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.batch_size = 16 #based on your hardware. 1GB per batch."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qg3U6LcmG2iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_bar = notebook.tqdm(\n",
        "    desc = 'training_routine',\n",
        "    total = args.num_epochs,\n",
        "    position=0,\n",
        "    leave = True,\n",
        ")\n",
        "dataset.set_split('train')\n",
        "train_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "dataset.set_split('val')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=eval',\n",
        "    total=dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_in'] = epoch_index\n",
        "\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 3, \n",
        "    )\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    model.train()\n",
        "\n",
        "    train_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.train()\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        y_pred = model(\n",
        "            input_ids = batch_dict['x_data'],\n",
        "            attention_mask =  batch_dict['x_attn_mask'],\n",
        "        )\n",
        "        y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "                             \n",
        "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "                             \n",
        "        loss_t = loss.item()\n",
        "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "                             \n",
        "        y_pred = y_pred.detach().cpu()\n",
        "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "        \n",
        "        acc_t = transformer_general_utils \\\n",
        "            .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "\n",
        "        f1_t = transformer_general_utils \\\n",
        "            .compute_macro_f1(y_pred, batch_dict['y_target'], average='weighted')\n",
        "\n",
        "        train_state['batch_preds'].append(y_pred)\n",
        "        train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "        train_state['batch_indexes'].append(batch_dict['x_index'])\n",
        "\n",
        "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "\n",
        "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                             epoch=epoch_index)\n",
        "\n",
        "        train_bar.update()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    train_state['train_accuracies'].append(running_acc)\n",
        "    train_state['train_losses'].append(running_loss)\n",
        "    \n",
        "    train_state['train_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "    train_state['train_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['train_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    train_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['train_preds'][-1],\n",
        "                                  train_state['train_targets'][-1],\n",
        "                                  'weighted'\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['train_f1s'].append(train_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    \n",
        "    dataset.set_split('val')\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = False, n_workers = 2, \n",
        "    )\n",
        "    eval_bar.reset(\n",
        "        total=dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    running_f1 = 0.0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        optimizer._backup_and_load_cache()\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(\n",
        "                input_ids = batch_dict['x_data'],\n",
        "                attention_mask =  batch_dict['x_attn_mask'],\n",
        "            )\n",
        "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            acc_t = transformer_general_utils\\\n",
        "                .compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            f1_t = transformer_general_utils \\\n",
        "                .compute_macro_f1(y_pred, batch_dict['y_target'],\n",
        "                                 average='weighted')\n",
        "\n",
        "            train_state['batch_preds'].append(y_pred.cpu())\n",
        "            train_state['batch_targets'].append(batch_dict['y_target'])\n",
        "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
        "            \n",
        "\n",
        "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
        "                                 epoch=epoch_index)\n",
        "            eval_bar.update()\n",
        "            \n",
        "    train_state['val_accuracies'].append(running_acc)\n",
        "    train_state['val_losses'].append(running_loss)\n",
        "    \n",
        "        \n",
        "    train_state['val_preds'].append(\n",
        "        torch.cat(train_state['batch_preds']).cpu()\n",
        "    )\n",
        "\n",
        "    train_state['val_targets'].append(\n",
        "        torch.cat(train_state['batch_targets']).cpu()\n",
        "    )\n",
        "    train_state['val_indexes'].append(\n",
        "        torch.cat(train_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    val_f1 = transformer_general_utils \\\n",
        "                .compute_macro_f1(train_state['val_preds'][-1],\n",
        "                                  train_state['val_targets'][-1],\n",
        "                                  average='weighted',\n",
        "                                 )\n",
        "                                 \n",
        "    train_state['val_f1s'].append(val_f1)\n",
        "    \n",
        "    train_state['batch_preds'] = []\n",
        "    train_state['batch_targets'] = []\n",
        "    train_state['batch_indexes'] = []\n",
        "    \n",
        "    torch.save(\n",
        "        {\n",
        "            'model':model.state_dict(),\n",
        "        },\n",
        "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
        "    )\n",
        "    \n",
        "    scheduler.step(val_f1)\n",
        "    early_stopping(val_f1, model)\n",
        "    optimizer._clear_and_load_backup()\n",
        "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1)\n",
        "    epoch_bar.update()    \n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1 )\n",
        "    epoch_bar.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X3cbb-1dG2iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_state['train_f1s'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoYjyEe1G2iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_state['val_f1s'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJePw9boG2iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrQhfw2iG2iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_run_index = train_state['val_f1s'].index(max(train_state['val_f1s']))\n",
        "print(f'Best run at epoch {best_run_index}')\n",
        "print('Train:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['train_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['train_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")\n",
        "print('Dev:',classification_report(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    digits=4)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1XaxsPG2ib",
        "colab_type": "text"
      },
      "source": [
        "## Check if ensembling helps and pick models to use on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOEv36m1G2ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_preds(indexes, preds):\n",
        "    \"\"\"Sorts the predictions in order, to reverse the effects of shuffle\n",
        "    done by dataloader\"\"\"\n",
        "    indexes = indexes.cpu().numpy().reshape(-1,1)\n",
        "    preds = preds.cpu().numpy()\n",
        "    arr_concat = np.hstack((indexes,preds)) #concat the preds and their indexes\n",
        "    sort_arr = arr_concat[ arr_concat[:,0].argsort()] #sort based on the indexes\n",
        "    sorted_preds = np.delete(sort_arr,0,axis=1)\n",
        "    return sorted_preds\n",
        "\n",
        "def get_optimal_models(train_state, split, reverse=False ):\n",
        "    \"\"\"Naive Ensembling\"\"\"\n",
        "    trgts= sort_preds(train_state[f'{split}_indexes'][-1],train_state[f'{split}_targets'][-1].reshape(-1,1))\n",
        "    total_preds = len(train_state[f'{split}_indexes'])\n",
        "    init = np.zeros(train_state[f'{split}_preds'][-1].shape)\n",
        "    max_f1 = 0\n",
        "    idxes = []\n",
        "    rng = range(0,total_preds)\n",
        "    if reverse:\n",
        "        rng = reversed(rng)\n",
        "    for i in rng:\n",
        "        temp = sort_preds(train_state[f'{split}_indexes'][i],train_state[f'{split}_preds'][i])\n",
        "        temp2 = init+temp\n",
        "        f1 = f1_score(\n",
        "            y_pred=temp2.argmax(axis=1),\n",
        "            y_true= trgts, average ='weighted'\n",
        "        )\n",
        "        if f1 > max_f1:\n",
        "            max_f1 = f1\n",
        "            init = init+temp\n",
        "            idxes.append(i)\n",
        "    print(f'Taking preds from {idxes} | Dev f1:{f1}')\n",
        "    return (idxes,max_f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDBXq6bCX5P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_state['val_f1s']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vWIVkH1yG2ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model_f1_score = f1_score(\n",
        "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
        "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
        "    average='weighted'\n",
        ")\n",
        "_models= [get_optimal_models(train_state,'val', reverse=False),\n",
        "                 get_optimal_models(train_state,'val', reverse=True),\n",
        "                 ([best_run_index],best_model_f1_score),]\n",
        "optimal_models = max(_models, key=lambda x:x[1]) #select ensembles or best model \n",
        "print(f'Optimal models chosen: {optimal_models}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l8TL72xSG2ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls {args.directory}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjIrLT7G2ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_models= [os.path.join(args.directory,i) for i in os.listdir(args.directory) if args.model_name in i]\n",
        "all_models = sorted(all_models, key = lambda x: int(x[8])) #sort by epoch num.\n",
        "all_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fya481n6G2im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected_models = [all_models[i] for i in optimal_models[0]]\n",
        "pprint.pprint(selected_models)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLdc2BW3G2iq",
        "colab_type": "text"
      },
      "source": [
        "## Loading test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZx05lCxG2ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_loc = '/content/trac2020_submission/data/test/trac2_hin_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN8JtlJNG2it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(test_set_loc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96k6QhTcG2iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['text'] = test_df['Text'].map(roberta_preproc.add_special_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qeXnNh0G2ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df['split'] = 'test'  #dummy label\n",
        "test_df['label'] = -1  #dummy label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p4nzTM8G2iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DanekyzcG2i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = TracDataset(\n",
        "    data_df = test_df,\n",
        "    tokenizer = xlmroberta_tokenizer,\n",
        "    max_seq_length = dataset._max_seq_length\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DoISn3hG2i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset.set_split('test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poqOKf2fG2i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset._target_df.split.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AQRrWaG2i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_state = general_utils.make_train_state() \n",
        "test_dataset.set_split('test')\n",
        "eval_bar = notebook.tqdm(\n",
        "    desc = 'split=train ',\n",
        "    total=test_dataset.get_num_batches(args.batch_size),\n",
        "    position=0,\n",
        "    leave=True,\n",
        ")\n",
        "model.eval()\n",
        "for m in notebook.tqdm(selected_models, total=len(selected_models)):\n",
        "    eval_bar.reset(\n",
        "        total=test_dataset.get_num_batches(args.batch_size),\n",
        "    )\n",
        "    model.load_state_dict(torch.load(m)['model'])\n",
        "    batch_generator = generate_batches(\n",
        "        dataset= test_dataset, batch_size= args.batch_size, shuffle=False,\n",
        "        device = args.device, drop_last=False,\n",
        "        pinned_memory = True, n_workers = 1, \n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            y_pred = model(\n",
        "                input_ids = batch_dict['x_data'],\n",
        "                attention_mask =  batch_dict['x_attn_mask'],\n",
        "            )\n",
        "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
        "            \n",
        "            y_pred = y_pred.detach()\n",
        "            \n",
        "            batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
        "            test_state['batch_preds'].append(y_pred.cpu())\n",
        "            test_state['batch_targets'].append(batch_dict['y_target'].cpu())\n",
        "            test_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
        "            eval_bar.update()\n",
        "\n",
        "    test_state['val_preds'].append(\n",
        "        torch.cat(test_state['batch_preds']).cpu()\n",
        "    )\n",
        "    test_state['val_targets'].append(\n",
        "        torch.cat(test_state['batch_targets']).cpu()\n",
        "    )\n",
        "    test_state['val_indexes'].append(\n",
        "        torch.cat(test_state['batch_indexes']).cpu()\n",
        "    )\n",
        "    \n",
        "    test_state['batch_preds'] = []\n",
        "    test_state['batch_targets'] = []\n",
        "    test_state['batch_indexes'] = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBLNs164G2i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(test_state['val_preds']) == len(optimal_models[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tPSiWo8bjPR",
        "colab_type": "text"
      },
      "source": [
        "### Add the last layer outputs and apply argmax "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiCZ-VyG2i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble = torch.zeros_like(test_state['val_preds'][-1])\n",
        "for i in test_state['val_preds']:\n",
        "    ensemble += i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqINUy7gG2jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = torch.argmax(ensemble, dim=1).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fctK1svvG2jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collections.Counter(test_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwD7ZE_G2jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# task_b_label_dict = {'NGEN':0, 'GEN':1} #ref Reading TRAC2020 data... ipynb\n",
        "int_to_label = {0:'NGEN', 1:'GEN'}\n",
        "pred_labels = [int_to_label[i] for i in test_preds]\n",
        "collections.Counter(pred_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nJahLYRG2jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame( data= {'id':test_df.ID, 'label':pred_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyQiODTBG2jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_analysis_df = pd.DataFrame( data= {'id':test_df.ID, 'text':test_df.Text ,'label':pred_labels})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1C3S0zUDG2jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ2RV3NWG2jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_analysis_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}