{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaHXdQweG2gY"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnXQrkMlG2ga"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UcEUgF4G2gc"
   },
   "source": [
    "At the time of our work, we used the following library versions\n",
    "- numpy 1.18.1\n",
    "- pandas 1.0.1\n",
    "- torch 1.2.0\n",
    "- Cuda 10.0\n",
    "- python 3.7.0\n",
    "- sklearn 0.22.1\n",
    "- tqdm 4.42.1\n",
    "- nltk 3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jknbt-9FG9sq"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cozek/trac2020_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tt7C7pQyHPLp"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!pip install /content/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmMKCJ-dG2ge"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/trac2020_submission/src/')\n",
    "import collections\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "import importlib\n",
    "import pprint\n",
    "import nltk\n",
    "import datetime\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWwTBxXqG2gh"
   },
   "outputs": [],
   "source": [
    "import utils.general as general_utils\n",
    "import utils.trac2020 as trac_utils\n",
    "import utils.transformer.data as transformer_data_utils\n",
    "import utils.transformer.general as transformer_general_utils\n",
    "general_utils.set_seed_everywhere() #set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EfdINgcG2gk"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k1nQ5d4G2gn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcFCAJAQG2gr"
   },
   "source": [
    "## Import Optimzer and XLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc66UeiyG2gs"
   },
   "outputs": [],
   "source": [
    "# Import RAdam and Lookahead\n",
    "from radam.radam import RAdam\n",
    "from lookahead.optimizer import Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sF7KP8tzG2gv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIAYRiqhG2gz"
   },
   "source": [
    "# Set up the argspace/important_variables\n",
    "Please note that performance is suseptible to hyper parameters. We used a Nvidia Tesla V100 32GB. If you lower the batch size or change any other parameters, modules to fit your machine, you might not get the same performance as reported in our paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F6tx_VrG2gz"
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "        #use cuda by default\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "        #set batch size and number of epochs\n",
    "        batch_size = 32,\n",
    "        num_epochs = 20,\n",
    "    \n",
    "        #set the learning rate\n",
    "        learning_rate = 0.0001,\n",
    "\n",
    "        #location of the train, dev and test csv\n",
    "        train_csv = '/content/trac2020_submission/data/hin/trac2_hin_train.csv',\n",
    "        dev_csv = '/content/trac2020_submission/data/hin/trac2_hin_dev.csv',\n",
    "        test_csv = '/content/trac2020_submission/data/test/trac2_hin_test.csv',\n",
    "    \n",
    "        #directory to save our models at\n",
    "        directory = './', \n",
    "        model_name = 'xlmrobeta_hin_b.pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HY29sw-7G2g2"
   },
   "source": [
    "## Load the data csv into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKKJ-JmZG2g3"
   },
   "outputs": [],
   "source": [
    "raw_train_df =  pd.read_csv(args.train_csv)\n",
    "raw_train_df['split'] = 'train'\n",
    "print(raw_train_df.columns)\n",
    "print(raw_train_df['Sub-task A'].value_counts())\n",
    "print(raw_train_df['Sub-task B'].value_counts())\n",
    "print(f\"Size of 'train' split: {len(raw_train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSM9RGfRG2g8"
   },
   "outputs": [],
   "source": [
    "raw_dev_df =  pd.read_csv(args.dev_csv)\n",
    "raw_dev_df['split'] = 'dev'\n",
    "print(raw_dev_df.columns)\n",
    "print(raw_dev_df['Sub-task A'].value_counts())\n",
    "print(raw_dev_df['Sub-task B'].value_counts())\n",
    "print(f\"Size of 'dev' split: {len(raw_dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W99Hc1Y-G2hA"
   },
   "outputs": [],
   "source": [
    "# Concatinate both train and dev dfs together\n",
    "data_df = pd.concat([raw_dev_df, raw_train_df], ignore_index= True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6z1dLUbuG2hD"
   },
   "source": [
    "### Samples given per label size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ad1zsEMPG2hE"
   },
   "outputs": [],
   "source": [
    "print(f'Total dev + train size = {len(data_df)}\\n')\n",
    "print(data_df['Sub-task A'].value_counts(),'\\n')\n",
    "print(data_df['Sub-task B'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hY0WnQuG2hH"
   },
   "source": [
    "### Map to labels to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqQas0q9G2hI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task_b_label_dict = {'NGEN':0, 'GEN':1}\n",
    "print(task_b_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7M9WRnMG2hL"
   },
   "source": [
    "### Renaming the columns for our torch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nzmWDI7G2hN"
   },
   "outputs": [],
   "source": [
    "data_df_task_b = data_df[['ID','Text','Sub-task B','split']].copy()\n",
    "data_df_task_b.columns.values[1] = 'text'\n",
    "data_df_task_b.columns.values[2] = 'label'\n",
    "data_df_task_b.loc[:,'label'] = data_df_task_b.loc[:,'label'].map(task_b_label_dict) \n",
    "data_df_task_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQxb1PR7G2hR"
   },
   "outputs": [],
   "source": [
    "print(\"Num samples per class\")\n",
    "print(data_df_task_b.label.value_counts())\n",
    "\n",
    "print(\"\\nNum samples per split\")\n",
    "print(data_df_task_b.split.value_counts())\n",
    "\n",
    "print(\"\\nLabel counts in dev split\")\n",
    "print(data_df_task_b[data_df_task_b.split=='dev'].label.value_counts())\n",
    "\n",
    "print(\"\\nLabel counts in train split\")\n",
    "print(data_df_task_b[data_df_task_b.split=='train'].label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYW_O7yfG2hU"
   },
   "source": [
    "### We split long samples into multiple samples\n",
    "Each sample produced from from a single split will have the label of the original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRIJwIb_G2hV"
   },
   "outputs": [],
   "source": [
    "#split long sentences into sentences of 200 words\n",
    "data_df_task_b['text'] = data_df_task_b['text'].map(lambda x: trac_utils.chunk_sent(x,150,50))\n",
    "exploded_df = data_df_task_b.explode('text').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YNRu4kwUG2hX"
   },
   "source": [
    "#### Notice how a single sample is split into two samples in the exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CX-WCP-G2hY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df_task_b[data_df_task_b.ID=='C7.849']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJ_6MLU9G2ha",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exploded_df[exploded_df.ID == 'C7.849']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZwuf3tqG2he",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Samples before splitting\")\n",
    "print(data_df_task_b.split.value_counts())\n",
    "\n",
    "print(\"\\nSamples After splitting\")\n",
    "print(exploded_df.split.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKy-82YrG2hh"
   },
   "source": [
    "## Create the text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyhSisD5G2hi"
   },
   "outputs": [],
   "source": [
    "class RobertaPreprocessor():\n",
    "    \"\"\"\n",
    "    Preprocessor for adding special tokens into each sample\n",
    "    NOTE: Doesn't work perfectly.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,transformer_tokenizer,sentence_detector):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transformer_tokenizer: Tokenizer for the transformer model\n",
    "            sentence_detector: Sentence tokenizer.\n",
    "        \"\"\"\n",
    "        self.transformer_tokenizer = transformer_tokenizer\n",
    "        self.sentence_detector = sentence_detector\n",
    "        self.bos_token = transformer_tokenizer.bos_token\n",
    "        self.sep_token = ' ' + transformer_tokenizer.sep_token + ' '\n",
    "        \n",
    "    def add_special_tokens(self, text):\n",
    "        \"\"\"\n",
    "        Adds '</s>' between each sentence and at the end of the sample.\n",
    "        Adds '<s>' at the start of the sentence.\n",
    "        \n",
    "        Args:\n",
    "            text: Text sample to add special tokens into\n",
    "        Returns:\n",
    "            text with special tokens added\n",
    "        \"\"\"\n",
    "        text = ' '.join(text.strip().split()) #clean whitespaces\n",
    "        sentences = self.sentence_detector.tokenize(text)\n",
    "        eos_added_text  = self.sep_token.join(sentences) \n",
    "        return self.bos_token +' '+ eos_added_text + ' ' + self.transformer_tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGCMVmYNJHca"
   },
   "outputs": [],
   "source": [
    "!python -c 'import nltk; nltk.download(\"punkt\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX6GH1WNG2hl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "punkt_sentence_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XZCJQllG2hn"
   },
   "outputs": [],
   "source": [
    "roberta_preproc = RobertaPreprocessor(xlmroberta_tokenizer, punkt_sentence_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKml6Hy5G2hp"
   },
   "outputs": [],
   "source": [
    "#apply the preprocessor on the exploded dataframe\n",
    "exploded_df['text'] = exploded_df['text'].map(roberta_preproc.add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRdNEy92G2hs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exploded_df.loc[2].text #notice the addition of eos token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43q7GSYEG2hu"
   },
   "source": [
    "### Create the Vectorizer and the torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlI6CoLfG2hv"
   },
   "outputs": [],
   "source": [
    "class SimpleVectorizer():\n",
    "    \"\"\"Vectorizes Class to encode the samples into \n",
    "    their token ids and creates their respective attention masks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,tokenizer: Callable, max_seq_len: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenizer (Callable): transformer tokenizer\n",
    "            max_seq_len (int): Maximum sequence lenght \n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self._max_seq_len = max_seq_len\n",
    "\n",
    "    def vectorize(self,text :str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text: Text sample to vectorize\n",
    "        Returns:\n",
    "            ids: Token ids of the \n",
    "            attn: Attention masks for ids \n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False, #already added by preprocessor\n",
    "            max_length = self._max_seq_len,\n",
    "            pad_to_max_length = True,\n",
    "        )\n",
    "        ids =  np.array(encoded['input_ids'], dtype=np.int64)\n",
    "        attn = np.array(encoded['attention_mask'], dtype=np.int64)\n",
    "        \n",
    "        return ids, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2wGyBHOG2h0"
   },
   "outputs": [],
   "source": [
    "class TracDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset class\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_df: pd.DataFrame,\n",
    "        tokenizer: Callable,\n",
    "        max_seq_length:int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_df (pandas.DataFrame): df containing the labels and text\n",
    "            tokenizer (Callable): tokenizer for the transformer\n",
    "            max_seq_length (int): Maximum sequece length to work with.\n",
    "        \"\"\"\n",
    "        self.data_df = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if max_seq_length is None:\n",
    "            self._max_seq_length = self._get_max_len(data_df,tokenizer)\n",
    "        else:\n",
    "            self._max_seq_length = max_seq_length\n",
    "\n",
    "        self.train_df = self.data_df[self.data_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.data_df[self.data_df.split == 'dev']\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.data_df[self.data_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._simple_vectorizer = SimpleVectorizer(tokenizer, self._max_seq_length)\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.val_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    \n",
    "    def _get_max_len(self,data_df: pd.DataFrame, tokenizer: Callable):\n",
    "        \"\"\"Get the maximum lenght found in the data\n",
    "        Args:\n",
    "            data_df (pandas.DataFrame): The pandas dataframe with the data\n",
    "            tokenizer (Callable): The tokenizer of the transformer\n",
    "        Returns:\n",
    "            max_len (int): Maximum length\n",
    "        \"\"\"\n",
    "        len_func = lambda x: len(self.tokenizer.encode_plus(x)['input_ids'])\n",
    "        max_len = data_df.text.map(len_func).max() \n",
    "        return max_len\n",
    "\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        \n",
    "        indices, attention_masks = self._simple_vectorizer.vectorize(row.text)\n",
    "\n",
    "\n",
    "        label = row.label\n",
    "        return {'x_data': indices,\n",
    "                'x_attn_mask': attention_masks,\n",
    "                'x_index': index,\n",
    "                'y_target': label}\n",
    "    \n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0izVXm7xG2h2"
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=False, device=\"cpu\", pinned_memory = False, n_workers = 0): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last,\n",
    "                            pin_memory= pinned_memory,\n",
    "                            num_workers = n_workers,\n",
    "                            )\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        out_data_dict['x_data'] = data_dict['x_data'].to(\n",
    "            device, non_blocking= (True if pinned_memory else False) \n",
    "        )\n",
    "        out_data_dict['x_attn_mask'] = data_dict['x_attn_mask'].to(\n",
    "            device, non_blocking= (True if pinned_memory else False) \n",
    "        )\n",
    "        out_data_dict['x_index'] = data_dict['x_index']\n",
    "        out_data_dict['y_target'] = data_dict['y_target'].to(\n",
    "            device, non_blocking= (True if pinned_memory else False) \n",
    "        )\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lctSoIHfG2h5"
   },
   "source": [
    "## Initialize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPG1JYbDG2h6"
   },
   "outputs": [],
   "source": [
    "dataset = TracDataset(\n",
    "    data_df = exploded_df,\n",
    "    tokenizer = xlmroberta_tokenizer,\n",
    "    max_seq_length = 403 #what we used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbulHEyLG2h8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset._max_seq_length # make sure its safe enough for our model, i,e, < 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWqPuxZvG2iA"
   },
   "source": [
    "# Creating the XLMRoberta + Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsL6TvrsG2iA"
   },
   "outputs": [],
   "source": [
    "class XLMRoBertAttention(nn.Module):\n",
    "    \"\"\"Implements Attention Head Classifier\n",
    "    on Pretrained Roberta Transformer representations.\n",
    "    Attention Head Implementation : https://www.aclweb.org/anthology/P16-2034/\n",
    "    \"\"\"\n",
    "    \n",
    "    def penalized_tanh(self,x):\n",
    "        \"\"\"\n",
    "        http://aclweb.org/anthology/D18-1472\n",
    "        \"\"\"\n",
    "        alpha = 0.25\n",
    "        return torch.max(torch.tanh(x), alpha*torch.tanh(x))\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_name, num_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: model name, eg, roberta-base'\n",
    "            num_labels: number of classes to classify\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(768,1, bias=False)\n",
    "        self.prediction_layer = nn.Linear(768, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes the weights of the Attention head classifier\"\"\"\n",
    "        \n",
    "        for name, param in self.prediction_layer.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        for name, param in self.w.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: sent encoded into indices\n",
    "            attention_mask: their respective attention masks\n",
    "        Returns:\n",
    "            preds: Final layer output of the model\n",
    "        \"\"\"\n",
    "        embeddings = self.bert(input_ids = input_ids,\n",
    "                  attention_mask = attention_mask)\n",
    "        H = embeddings[0] #final hidden layer outputs \n",
    "        M = self.penalized_tanh(H)\n",
    "        alpha = torch.softmax(self.w(M), dim=1)\n",
    "        r = torch.bmm(H.permute(0,2,1),alpha)\n",
    "        h_star = self.penalized_tanh(r)\n",
    "        preds = self.prediction_layer(h_star.permute(0,2,1))\n",
    "        return preds\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78_xW4uUG2iD"
   },
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6NjugAyG2iE",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = XLMRoBertAttention(\n",
    "    model_name = 'xlm-roberta-base',\n",
    "    num_labels = len(set(dataset.data_df.label)),\n",
    ")\n",
    "model.to(args.device) #send the model to the 'cpu' or 'gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvKmZUh_G2iH",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "early_stopping = transformer_general_utils.EarlyStopping(patience=4)\n",
    "base_optimizer = RAdam(model.parameters(), lr = args.learning_rate, weight_decay=1e-5)\n",
    "optimizer = Lookahead(optimizer = base_optimizer, k = 6, alpha=0.5 )\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer.optimizer, factor =0.1 ,mode='max')\n",
    "\n",
    "print(f'Using LR:{args.learning_rate}\\n Early Stopping Patience: 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLAYZmmtG2iJ"
   },
   "source": [
    "# Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vC3ewUSnG2iK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_state = general_utils.make_train_state() #dictionary for saving training routine information\n",
    "train_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lr0zwdnYKNtf"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pc81HVEZKWzO"
   },
   "outputs": [],
   "source": [
    "args.batch_size = 16 #based on your hardware. 1GB per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qg3U6LcmG2iN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_bar = notebook.tqdm(\n",
    "    desc = 'training_routine',\n",
    "    total = args.num_epochs,\n",
    "    position=0,\n",
    "    leave = True,\n",
    ")\n",
    "dataset.set_split('train')\n",
    "train_bar = notebook.tqdm(\n",
    "    desc = 'split=train ',\n",
    "    total=dataset.get_num_batches(args.batch_size),\n",
    "    position=0,\n",
    "    leave=True,\n",
    ")\n",
    "dataset.set_split('val')\n",
    "eval_bar = notebook.tqdm(\n",
    "    desc = 'split=eval',\n",
    "    total=dataset.get_num_batches(args.batch_size),\n",
    "    position=0,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_in'] = epoch_index\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(\n",
    "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
    "        device = args.device, drop_last=False,\n",
    "        pinned_memory = True, n_workers = 3, \n",
    "    )\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_f1 = 0.0\n",
    "    model.train()\n",
    "\n",
    "    train_bar.reset(\n",
    "        total=dataset.get_num_batches(args.batch_size),\n",
    "    )\n",
    "    model.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(\n",
    "            input_ids = batch_dict['x_data'],\n",
    "            attention_mask =  batch_dict['x_attn_mask'],\n",
    "        )\n",
    "        y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
    "                             \n",
    "        loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                             \n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "                             \n",
    "        y_pred = y_pred.detach().cpu()\n",
    "        batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
    "        \n",
    "        acc_t = transformer_general_utils \\\n",
    "            .compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "\n",
    "        f1_t = transformer_general_utils \\\n",
    "            .compute_macro_f1(y_pred, batch_dict['y_target'], average='weighted')\n",
    "\n",
    "        train_state['batch_preds'].append(y_pred)\n",
    "        train_state['batch_targets'].append(batch_dict['y_target'])\n",
    "        train_state['batch_indexes'].append(batch_dict['x_index'])\n",
    "\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
    "                             epoch=epoch_index)\n",
    "\n",
    "        train_bar.update()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    train_state['train_accuracies'].append(running_acc)\n",
    "    train_state['train_losses'].append(running_loss)\n",
    "    \n",
    "    train_state['train_preds'].append(\n",
    "        torch.cat(train_state['batch_preds']).cpu()\n",
    "    )\n",
    "    train_state['train_targets'].append(\n",
    "        torch.cat(train_state['batch_targets']).cpu()\n",
    "    )\n",
    "    train_state['train_indexes'].append(\n",
    "        torch.cat(train_state['batch_indexes']).cpu()\n",
    "    )\n",
    "    train_f1 = transformer_general_utils \\\n",
    "                .compute_macro_f1(train_state['train_preds'][-1],\n",
    "                                  train_state['train_targets'][-1],\n",
    "                                  'weighted'\n",
    "                                 )\n",
    "                                 \n",
    "    train_state['train_f1s'].append(train_f1)\n",
    "    \n",
    "    train_state['batch_preds'] = []\n",
    "    train_state['batch_targets'] = []\n",
    "    train_state['batch_indexes'] = []\n",
    "    \n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(\n",
    "        dataset= dataset, batch_size= args.batch_size, shuffle=True,\n",
    "        device = args.device, drop_last=False,\n",
    "        pinned_memory = False, n_workers = 2, \n",
    "    )\n",
    "    eval_bar.reset(\n",
    "        total=dataset.get_num_batches(args.batch_size),\n",
    "    )\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_f1 = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        optimizer._backup_and_load_cache()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = model(\n",
    "                input_ids = batch_dict['x_data'],\n",
    "                attention_mask =  batch_dict['x_attn_mask'],\n",
    "            )\n",
    "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
    "\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            y_pred = y_pred.detach()\n",
    "            \n",
    "            acc_t = transformer_general_utils\\\n",
    "                .compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            f1_t = transformer_general_utils \\\n",
    "                .compute_macro_f1(y_pred, batch_dict['y_target'],\n",
    "                                 average='weighted')\n",
    "\n",
    "            train_state['batch_preds'].append(y_pred.cpu())\n",
    "            train_state['batch_targets'].append(batch_dict['y_target'])\n",
    "            train_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
    "\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
    "            \n",
    "\n",
    "            eval_bar.set_postfix(loss = running_loss, f1 = running_f1, acc=running_acc,\n",
    "                                 epoch=epoch_index)\n",
    "            eval_bar.update()\n",
    "            \n",
    "    train_state['val_accuracies'].append(running_acc)\n",
    "    train_state['val_losses'].append(running_loss)\n",
    "    \n",
    "        \n",
    "    train_state['val_preds'].append(\n",
    "        torch.cat(train_state['batch_preds']).cpu()\n",
    "    )\n",
    "\n",
    "    train_state['val_targets'].append(\n",
    "        torch.cat(train_state['batch_targets']).cpu()\n",
    "    )\n",
    "    train_state['val_indexes'].append(\n",
    "        torch.cat(train_state['batch_indexes']).cpu()\n",
    "    )\n",
    "    val_f1 = transformer_general_utils \\\n",
    "                .compute_macro_f1(train_state['val_preds'][-1],\n",
    "                                  train_state['val_targets'][-1],\n",
    "                                  average='weighted',\n",
    "                                 )\n",
    "                                 \n",
    "    train_state['val_f1s'].append(val_f1)\n",
    "    \n",
    "    train_state['batch_preds'] = []\n",
    "    train_state['batch_targets'] = []\n",
    "    train_state['batch_indexes'] = []\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'model':model.state_dict(),\n",
    "        },\n",
    "        args.directory + f'_epoc_{epoch_index}_' + args.model_name,\n",
    "    )\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    early_stopping(val_f1, model)\n",
    "    optimizer._clear_and_load_backup()\n",
    "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1)\n",
    "    epoch_bar.update()    \n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    epoch_bar.set_postfix( best_f1 = early_stopping.best_score, current = val_f1 )\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3cbb-1dG2iQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_state['train_f1s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoYjyEe1G2iS"
   },
   "outputs": [],
   "source": [
    "print(train_state['val_f1s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJePw9boG2iU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrQhfw2iG2iY"
   },
   "outputs": [],
   "source": [
    "best_run_index = train_state['val_f1s'].index(max(train_state['val_f1s']))\n",
    "print(f'Best run at epoch {best_run_index}')\n",
    "print('Train:',classification_report(\n",
    "    y_pred=(torch.argmax(train_state['train_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
    "    y_true= train_state['train_targets'][best_run_index].cpu().numpy(), \n",
    "    digits=4)\n",
    ")\n",
    "print('Dev:',classification_report(\n",
    "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
    "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
    "    digits=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jI1XaxsPG2ib"
   },
   "source": [
    "## Check if ensembling helps and pick models to use on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOEv36m1G2ib"
   },
   "outputs": [],
   "source": [
    "def sort_preds(indexes, preds):\n",
    "    \"\"\"Sorts the predictions in order, to reverse the effects of shuffle\n",
    "    done by dataloader\"\"\"\n",
    "    indexes = indexes.cpu().numpy().reshape(-1,1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    arr_concat = np.hstack((indexes,preds)) #concat the preds and their indexes\n",
    "    sort_arr = arr_concat[ arr_concat[:,0].argsort()] #sort based on the indexes\n",
    "    sorted_preds = np.delete(sort_arr,0,axis=1)\n",
    "    return sorted_preds\n",
    "\n",
    "def get_optimal_models(train_state, split, reverse=False ):\n",
    "    \"\"\"Naive Ensembling\"\"\"\n",
    "    trgts= sort_preds(train_state[f'{split}_indexes'][-1],train_state[f'{split}_targets'][-1].reshape(-1,1))\n",
    "    total_preds = len(train_state[f'{split}_indexes'])\n",
    "    init = np.zeros(train_state[f'{split}_preds'][-1].shape)\n",
    "    max_f1 = 0\n",
    "    idxes = []\n",
    "    rng = range(0,total_preds)\n",
    "    if reverse:\n",
    "        rng = reversed(rng)\n",
    "    for i in rng:\n",
    "        temp = sort_preds(train_state[f'{split}_indexes'][i],train_state[f'{split}_preds'][i])\n",
    "        temp2 = init+temp\n",
    "        f1 = f1_score(\n",
    "            y_pred=temp2.argmax(axis=1),\n",
    "            y_true= trgts, average ='weighted'\n",
    "        )\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            init = init+temp\n",
    "            idxes.append(i)\n",
    "    print(f'Taking preds from {idxes} | Dev f1:{f1}')\n",
    "    return (idxes,max_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDBXq6bCX5P_"
   },
   "outputs": [],
   "source": [
    "train_state['val_f1s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWIVkH1yG2ie",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model_f1_score = f1_score(\n",
    "    y_pred=(torch.argmax(train_state['val_preds'][best_run_index],dim=1) ).cpu().long().numpy(),\n",
    "    y_true= train_state['val_targets'][best_run_index].cpu().numpy(), \n",
    "    average='weighted'\n",
    ")\n",
    "_models= [get_optimal_models(train_state,'val', reverse=False),\n",
    "                 get_optimal_models(train_state,'val', reverse=True),\n",
    "                 ([best_run_index],best_model_f1_score),]\n",
    "optimal_models = max(_models, key=lambda x:x[1]) #select ensembles or best model \n",
    "print(f'Optimal models chosen: {optimal_models}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8TL72xSG2ih",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls {args.directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpjIrLT7G2ik"
   },
   "outputs": [],
   "source": [
    "all_models= [os.path.join(args.directory,i) for i in os.listdir(args.directory) if args.model_name in i]\n",
    "all_models = sorted(all_models, key = lambda x: int(x[8])) #sort by epoch num.\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fya481n6G2im"
   },
   "outputs": [],
   "source": [
    "selected_models = [all_models[i] for i in optimal_models[0]]\n",
    "pprint.pprint(selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLdc2BW3G2iq"
   },
   "source": [
    "## Loading test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZx05lCxG2ir"
   },
   "outputs": [],
   "source": [
    "test_set_loc = '/content/trac2020_submission/data/test/trac2_hin_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN8JtlJNG2it"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_set_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96k6QhTcG2iv"
   },
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['Text'].map(roberta_preproc.add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qeXnNh0G2ix"
   },
   "outputs": [],
   "source": [
    "test_df['split'] = 'test'  #dummy label\n",
    "test_df['label'] = -1  #dummy label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3p4nzTM8G2iz"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DanekyzcG2i1"
   },
   "outputs": [],
   "source": [
    "test_dataset = TracDataset(\n",
    "    data_df = test_df,\n",
    "    tokenizer = xlmroberta_tokenizer,\n",
    "    max_seq_length = dataset._max_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DoISn3hG2i3"
   },
   "outputs": [],
   "source": [
    "test_dataset.set_split('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poqOKf2fG2i4"
   },
   "outputs": [],
   "source": [
    "test_dataset._target_df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5AQRrWaG2i6"
   },
   "outputs": [],
   "source": [
    "test_state = general_utils.make_train_state() \n",
    "test_dataset.set_split('test')\n",
    "eval_bar = notebook.tqdm(\n",
    "    desc = 'split=train ',\n",
    "    total=test_dataset.get_num_batches(args.batch_size),\n",
    "    position=0,\n",
    "    leave=True,\n",
    ")\n",
    "model.eval()\n",
    "for m in notebook.tqdm(selected_models, total=len(selected_models)):\n",
    "    eval_bar.reset(\n",
    "        total=test_dataset.get_num_batches(args.batch_size),\n",
    "    )\n",
    "    model.load_state_dict(torch.load(m)['model'])\n",
    "    batch_generator = generate_batches(\n",
    "        dataset= test_dataset, batch_size= args.batch_size, shuffle=False,\n",
    "        device = args.device, drop_last=False,\n",
    "        pinned_memory = True, n_workers = 1, \n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = model(\n",
    "                input_ids = batch_dict['x_data'],\n",
    "                attention_mask =  batch_dict['x_attn_mask'],\n",
    "            )\n",
    "            y_pred = y_pred.view(-1, len(set(dataset.data_df.label)))\n",
    "            \n",
    "            y_pred = y_pred.detach()\n",
    "            \n",
    "            batch_dict['y_target'] = batch_dict['y_target'].cpu()\n",
    "            test_state['batch_preds'].append(y_pred.cpu())\n",
    "            test_state['batch_targets'].append(batch_dict['y_target'].cpu())\n",
    "            test_state['batch_indexes'].append(batch_dict['x_index'].cpu())\n",
    "            eval_bar.update()\n",
    "\n",
    "    test_state['val_preds'].append(\n",
    "        torch.cat(test_state['batch_preds']).cpu()\n",
    "    )\n",
    "    test_state['val_targets'].append(\n",
    "        torch.cat(test_state['batch_targets']).cpu()\n",
    "    )\n",
    "    test_state['val_indexes'].append(\n",
    "        torch.cat(test_state['batch_indexes']).cpu()\n",
    "    )\n",
    "    \n",
    "    test_state['batch_preds'] = []\n",
    "    test_state['batch_targets'] = []\n",
    "    test_state['batch_indexes'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBLNs164G2i7"
   },
   "outputs": [],
   "source": [
    "assert len(test_state['val_preds']) == len(optimal_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tPSiWo8bjPR"
   },
   "source": [
    "### Add the last layer outputs and apply argmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiiCZ-VyG2i-"
   },
   "outputs": [],
   "source": [
    "ensemble = torch.zeros_like(test_state['val_preds'][-1])\n",
    "for i in test_state['val_preds']:\n",
    "    ensemble += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZqINUy7gG2jD"
   },
   "outputs": [],
   "source": [
    "test_preds = torch.argmax(ensemble, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fctK1svvG2jF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collections.Counter(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrwD7ZE_G2jI"
   },
   "outputs": [],
   "source": [
    "# task_b_label_dict = {'NGEN':0, 'GEN':1} #ref Reading TRAC2020 data... ipynb\n",
    "int_to_label = {0:'NGEN', 1:'GEN'}\n",
    "pred_labels = [int_to_label[i] for i in test_preds]\n",
    "collections.Counter(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nJahLYRG2jJ"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame( data= {'id':test_df.ID, 'label':pred_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyQiODTBG2jL"
   },
   "outputs": [],
   "source": [
    "pred_analysis_df = pd.DataFrame( data= {'id':test_df.ID, 'text':test_df.Text ,'label':pred_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1C3S0zUDG2jN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZ2RV3NWG2jO"
   },
   "outputs": [],
   "source": [
    "pred_analysis_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Hindi Task B - Ensemble XLMRobertaAttention .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
